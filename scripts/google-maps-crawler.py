#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ¸ SCENT DESTINATION Google Maps ì´ë¯¸ì§€ í¬ë¡¤ëŸ¬
Seleniumìœ¼ë¡œ êµ¬ê¸€ ì§€ë„ì—ì„œ ì§ì ‘ ì´ë¯¸ì§€ ìˆ˜ì§‘í•˜ëŠ” ì™„ì „ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import time
import json
import requests
import pandas as pd
from pathlib import Path
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class GoogleMapsImageCrawler:
    def __init__(self, headless=True, download_dir='public/images/places'):
        """
        êµ¬ê¸€ ì§€ë„ ì´ë¯¸ì§€ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            headless (bool): ë¸Œë¼ìš°ì €ë¥¼ ìˆ¨ê¹€ ëª¨ë“œë¡œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
            download_dir (str): ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        self.driver = self._setup_driver(headless)
        self.wait = WebDriverWait(self.driver, 10)
        
        print(f"ğŸš€ Google Maps ì´ë¯¸ì§€ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬: {self.download_dir.absolute()}")
    
    def _setup_driver(self, headless=True):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        options = Options()
        
        if headless:
            options.add_argument('--headless')
        
        # ê¸°ë³¸ ì˜µì…˜ë“¤
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent ì„¤ì •
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            return driver
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print("1. Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
            print("2. pip install webdriver-manager í›„ ìë™ ì„¤ì¹˜ ì‚¬ìš©")
            sys.exit(1)
    
    def search_place(self, place_name, region=""):
        """
        êµ¬ê¸€ ì§€ë„ì—ì„œ ì¥ì†Œ ê²€ìƒ‰
        
        Args:
            place_name (str): ê²€ìƒ‰í•  ì¥ì†Œëª…
            region (str): ì§€ì—­ëª… (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒìš©)
        
        Returns:
            bool: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # êµ¬ê¸€ ì§€ë„ ì ‘ì†
            self.driver.get('https://www.google.com/maps/')
            time.sleep(2)
            
            # ê²€ìƒ‰ì–´ ì¡°í•©
            search_query = f"{place_name} {region}".strip()
            print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {search_query}")
            
            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ì…ë ¥
            search_box = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'input#searchboxinput'))
            )
            search_box.clear()
            search_box.send_keys(search_query)
            search_box.send_keys(Keys.RETURN)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            return True
            
        except TimeoutException:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ")
            return False
        except Exception as e:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_place_images(self, max_images=3):
        """
        ê²€ìƒ‰ëœ ì¥ì†Œì˜ ì´ë¯¸ì§€ URLë“¤ ìˆ˜ì§‘
        
        Args:
            max_images (int): ìˆ˜ì§‘í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜
            
        Returns:
            list: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
        """
        image_urls = []
        
        try:
            # ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ í´ë¦­
            first_result = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, '[data-result-index="1"]'))
            )
            first_result.click()
            time.sleep(3)
            
            # ì´ë¯¸ì§€ ì„¹ì…˜ìœ¼ë¡œ ì´ë™
            try:
                # ì‚¬ì§„ íƒ­ ì°¾ê¸°
                photos_button = self.driver.find_element(By.XPATH, '//button[contains(@data-tab-index, "1")]')
                photos_button.click()
                time.sleep(2)
            except NoSuchElementException:
                print("ğŸ“· ì‚¬ì§„ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œë„")
            
            # ì´ë¯¸ì§€ ìš”ì†Œë“¤ ì°¾ê¸°
            image_elements = self.driver.find_elements(By.CSS_SELECTOR, 'img[src*="googleusercontent.com"]')
            
            for i, img_element in enumerate(image_elements[:max_images]):
                try:
                    img_url = img_element.get_attribute('src')
                    if img_url and 'googleusercontent.com' in img_url:
                        # ê³ í•´ìƒë„ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
                        if '=w' in img_url:
                            img_url = img_url.split('=w')[0] + '=w1200-h800'
                        image_urls.append(img_url)
                        print(f"  ğŸ“¸ ì´ë¯¸ì§€ {i+1} URL ìˆ˜ì§‘: {img_url[:80]}...")
                        
                except Exception as e:
                    print(f"  âš ï¸ ì´ë¯¸ì§€ {i+1} URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"âœ… ì´ {len(image_urls)}ê°œ ì´ë¯¸ì§€ URL ìˆ˜ì§‘")
            return image_urls
            
        except TimeoutException:
            print("âŒ ì¥ì†Œ ì •ë³´ ë¡œë”© ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ")
            return []
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def download_image(self, img_url, file_path):
        """
        ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        
        Args:
            img_url (str): ì´ë¯¸ì§€ URL
            file_path (Path): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            bool: ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(img_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {file_path.name}")
            return True
            
        except Exception as e:
            print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def crawl_place_images(self, place_name, region="", english_name="", max_images=3):
        """
        íŠ¹ì • ì¥ì†Œì˜ ì´ë¯¸ì§€ë“¤ì„ í¬ë¡¤ë§í•˜ê³  ì €ì¥
        
        Args:
            place_name (str): í•œê¸€ ì¥ì†Œëª…
            region (str): ì§€ì—­ëª…
            english_name (str): ì˜ë¬¸ íŒŒì¼ëª…
            max_images (int): ìˆ˜ì§‘í•  ì´ë¯¸ì§€ ìˆ˜
            
        Returns:
            int: ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ ìˆ˜
        """
        print(f"\nğŸ¯ '{place_name}' ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        if not self.search_place(place_name, region):
            return 0
        
        # ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        image_urls = self.get_place_images(max_images)
        if not image_urls:
            print(f"âŒ '{place_name}' ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return 0
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        success_count = 0
        for i, img_url in enumerate(image_urls):
            file_name = f"{english_name}-{i+1}.jpg"
            file_path = self.download_dir / file_name
            
            if self.download_image(img_url, file_path):
                success_count += 1
            
            # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
            time.sleep(1)
        
        print(f"âœ¨ '{place_name}' ì™„ë£Œ: {success_count}/{len(image_urls)} ì´ë¯¸ì§€ ì €ì¥")
        return success_count
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”š í¬ë¡¤ëŸ¬ ì¢…ë£Œ")

def load_spaces_data():
    """
    ê³µê°„ ë°ì´í„° ë¡œë“œ
    """
    # ì‹¤ì œ í”„ë¡œì íŠ¸ ë°ì´í„° + ì´ë¯¸ì§€ ê°€ì´ë“œì˜ ëª¨ë“  ê³µê°„ë“¤
    spaces_data = [
        # ì„œìš¸ ì§€ì—­
        {"name": "Osechill", "region": "ì„œìš¸", "english_name": "osechill"},
        {"name": "Aya Coffee", "region": "ì„œìš¸", "english_name": "aya-coffee"},
        {"name": "ì—°ë‚¨ì„œì‹", "region": "ì„œìš¸", "english_name": "yeonnam-seosik"},
        {"name": "ì• ê²½ ì•¤íŠ¸ëŸ¬ì‚¬ì´íŠ¸", "region": "ì„œìš¸", "english_name": "aekyung-anthracite"},
        {"name": "ì¹´í˜ ì˜¨ë¦¬", "region": "ì„œìš¸", "english_name": "cafe-only"},
        {"name": "ë¸”ë£¨ë³´í‹€ ì²­ë‹´", "region": "ì„œìš¸", "english_name": "bluebottle-cheongdam"},
        {"name": "ëŒ€ë¦¼ì°½ê³ ", "region": "ì„œìš¸", "english_name": "daelim-warehouse"},
        {"name": "ì–´ë‹ˆì–¸", "region": "ì„œìš¸", "english_name": "onion"},
        {"name": "í…Œë¼ë¡œì‚¬", "region": "ì„œìš¸", "english_name": "terarosa"},
        {"name": "ì†Œìš¸ì»´íŒ©íŠ¸", "region": "ì„œìš¸", "english_name": "soul-compact"},
        
        # ì œì£¼ ì§€ì—­
        {"name": "ëª½ìƒë“œì• ì›”", "region": "ì œì£¼", "english_name": "monsant-aewol"},
        {"name": "ì›ì•¤ì˜¨ë¦¬", "region": "ì œì£¼", "english_name": "one-and-only"},
        {"name": "ì¹´í˜ í•œë¼ì‚°", "region": "ì œì£¼", "english_name": "cafe-hallasan"},
        {"name": "ëª…ì›”ì´ˆë“±í•™êµ", "region": "ì œì£¼", "english_name": "myeongwol-school"},
        {"name": "í”Œë¡œì›¨ì´ë¸Œ", "region": "ì œì£¼", "english_name": "flowave"},
        {"name": "ì œì£¼ ì• ì›” ë´„ë‚ ì¹´í˜", "region": "ì œì£¼", "english_name": "jeju-aewol-bomnal"},
        {"name": "ì¹´í˜ ë¸ë¬¸ë„", "region": "ì œì£¼", "english_name": "cafe-delmundo"},
        {"name": "ì¹´í˜ ê¼¼ë§ˆ", "region": "ì œì£¼", "english_name": "cafe-comma"},
        {"name": "ì´ë‹ˆìŠ¤í”„ë¦¬ í•˜ìš°ìŠ¤", "region": "ì œì£¼", "english_name": "innisfree-house"},
        {"name": "ì˜¤ì„¤ë¡ í‹°ë®¤ì§€ì—„", "region": "ì œì£¼", "english_name": "osulloc-tea-museum"},
        
        # ë¶€ì‚° ì§€ì—­
        {"name": "í°ì—¬ìš¸ë¬¸í™”ë§ˆì„", "region": "ë¶€ì‚°", "english_name": "huinyeoul-village"},
        {"name": "ê°ì²œë¬¸í™”ë§ˆì„", "region": "ë¶€ì‚°", "english_name": "gamcheon-village"},
        {"name": "í•´ë™ìš©ê¶ì‚¬", "region": "ë¶€ì‚°", "english_name": "haedong-yonggungsa"},
        {"name": "íƒœì¢…ëŒ€", "region": "ë¶€ì‚°", "english_name": "taejongdae"},
        {"name": "ê´‘ì•ˆë¦¬í•´ë³€", "region": "ë¶€ì‚°", "english_name": "gwangalli-beach"},
        {"name": "ë¶€ì‚°íƒ€ì›Œ", "region": "ë¶€ì‚°", "english_name": "busan-tower"},
        {"name": "ìê°ˆì¹˜ì‹œì¥", "region": "ë¶€ì‚°", "english_name": "jagalchi-market"},
        {"name": "ì„¼í…€ì‹œí‹°", "region": "ë¶€ì‚°", "english_name": "centum-city"},
        
        # ê°•ë¦‰ ì§€ì—­
        {"name": "ê°•ë¦‰ ì•ˆëª©í•´ë³€", "region": "ê°•ë¦‰", "english_name": "anmok-beach"},
        {"name": "ê²½í¬í•´ë³€", "region": "ê°•ë¦‰", "english_name": "gyeongpo-beach"},
        {"name": "ì •ë™ì§„", "region": "ê°•ë¦‰", "english_name": "jeongdongjin"},
        {"name": "ì˜¤ì£½í—Œ", "region": "ê°•ë¦‰", "english_name": "ojukheon"},
        {"name": "í…Œë¼ë¡œì‚¬ ê°•ë¦‰", "region": "ê°•ë¦‰", "english_name": "terarosa-gangneung"},
        
        # ì „ì£¼ ì§€ì—­
        {"name": "ì „ì£¼í•œì˜¥ë§ˆì„", "region": "ì „ì£¼", "english_name": "jeonju-hanok-village"},
        {"name": "ê²½ê¸°ì „", "region": "ì „ì£¼", "english_name": "gyeonggijeon"},
        {"name": "ì˜¤ëª©ëŒ€", "region": "ì „ì£¼", "english_name": "omokdae"},
        {"name": "ì „ë™ì„±ë‹¹", "region": "ì „ì£¼", "english_name": "jeondong-cathedral"},
        
        # ê²½ì£¼ ì§€ì—­
        {"name": "ë¶ˆêµ­ì‚¬", "region": "ê²½ì£¼", "english_name": "bulguksa"},
        {"name": "ì„êµ´ì•”", "region": "ê²½ì£¼", "english_name": "seokguram"},
        {"name": "ì²¨ì„±ëŒ€", "region": "ê²½ì£¼", "english_name": "cheomseongdae"},
        {"name": "ì•ˆì••ì§€", "region": "ê²½ì£¼", "english_name": "anapji"},
        {"name": "ëŒ€ë¦‰ì›", "region": "ê²½ì£¼", "english_name": "daereungwon"},
        
        # ì—¬ìˆ˜ ì§€ì—­
        {"name": "ì—¬ìˆ˜ ë°¤ë°”ë‹¤", "region": "ì—¬ìˆ˜", "english_name": "yeosu-night-sea"},
        {"name": "ì˜¤ë™ë„", "region": "ì—¬ìˆ˜", "english_name": "odongdo"},
        {"name": "ì—¬ìˆ˜ì—‘ìŠ¤í¬", "region": "ì—¬ìˆ˜", "english_name": "yeosu-expo"},
        {"name": "í•˜ë©œë“±ëŒ€", "region": "ì—¬ìˆ˜", "english_name": "hamel-lighthouse"},
        
        # ê¸°íƒ€ ê´€ê´‘ì§€
        {"name": "ë‚¨ì´ì„¬", "region": "ê°€í‰", "english_name": "nami-island"},
        {"name": "ì¸ì‚¬ë™", "region": "ì„œìš¸", "english_name": "insadong"},
        {"name": "ëª…ë™", "region": "ì„œìš¸", "english_name": "myeongdong"},
        {"name": "í™ëŒ€", "region": "ì„œìš¸", "english_name": "hongdae"},
        {"name": "ì´íƒœì›", "region": "ì„œìš¸", "english_name": "itaewon"},
        {"name": "ê°•ë‚¨", "region": "ì„œìš¸", "english_name": "gangnam"},
        
        # ì¶”ê°€ ì§€ì—­ë“¤
        {"name": "ì†ì´ˆí•´ë³€", "region": "ì†ì´ˆ", "english_name": "sokcho-beach"},
        {"name": "ì„¤ì•…ì‚°", "region": "ì†ì´ˆ", "english_name": "seoraksan"},
        {"name": "ì†ì´ˆì¤‘ì•™ì‹œì¥", "region": "ì†ì´ˆ", "english_name": "sokcho-jungang-market"},
        {"name": "ì•„ë°”ì´ë§ˆì„", "region": "ì†ì´ˆ", "english_name": "abai-village"},
        
        {"name": "ë™í”¼ë‘ë²½í™”ë§ˆì„", "region": "í†µì˜", "english_name": "dongpirang-village"},
        {"name": "ë£¨ì§€", "region": "í†µì˜", "english_name": "luge"},
        {"name": "ì¼€ì´ë¸”ì¹´", "region": "í†µì˜", "english_name": "cable-car"},
        {"name": "í•œì‚°ë„", "region": "í†µì˜", "english_name": "hansando"},
        
        {"name": "ì£½ë…¹ì›", "region": "ë‹´ì–‘", "english_name": "juknokwon"},
        {"name": "ë©”íƒ€ì„¸ì¿¼ì´ì•„ê¸¸", "region": "ë‹´ì–‘", "english_name": "metasequoia-road"},
        {"name": "ê´€ë°©ì œë¦¼", "region": "ë‹´ì–‘", "english_name": "gwanbangjerim"},
        
        {"name": "í•˜íšŒë§ˆì„", "region": "ì•ˆë™", "english_name": "hahoe-village"},
        {"name": "ë„ì‚°ì„œì›", "region": "ì•ˆë™", "english_name": "dosan-seowon"},
        {"name": "ì•ˆë™ ê°„ê³ ë“±ì–´", "region": "ì•ˆë™", "english_name": "andong-mackerel"},
        
        {"name": "ì°¸ì†Œë¦¬ë°•ë¬¼ê´€", "region": "ê°•ë¦‰", "english_name": "chamsori-museum"},
        {"name": "ì»¤í”¼ì»¤í¼", "region": "ê°•ë¦‰", "english_name": "coffee-cupper"},
        {"name": "í•œêµ­ì „í†µë¬¸í™”ì „ë‹¹", "region": "ì „ì£¼", "english_name": "korean-traditional-culture-center"},
        {"name": "êµ­ë¦½ê²½ì£¼ë°•ë¬¼ê´€", "region": "ê²½ì£¼", "english_name": "gyeongju-national-museum"},
        {"name": "ëŒì‚°ëŒ€êµ", "region": "ì—¬ìˆ˜", "english_name": "dolsan-bridge"},
        {"name": "ì†¡ë„í•´ë³€", "region": "ë¶€ì‚°", "english_name": "songdo-beach"},
        {"name": "ì˜¤ë¥™ë„", "region": "ë¶€ì‚°", "english_name": "oryukdo"},
    ]
    
    print(f"âœ… {len(spaces_data)}ê°œ ê³µê°„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    return spaces_data

def run_automated_crawling():
    """ì „ì²´ ìë™í™” í¬ë¡¤ë§ ì‹¤í–‰"""
    print("ğŸš€ SCENT DESTINATION ì´ë¯¸ì§€ ìë™ í¬ë¡¤ë§ ì‹œì‘!")
    
    # ê³µê°„ ë°ì´í„° ë¡œë“œ
    spaces_data = load_spaces_data()
    total_spaces = len(spaces_data)
    
    print(f"ğŸ“Š ì´ {total_spaces}ê°œ ê³µê°„ Ã— 3ì¥ = {total_spaces * 3}ì¥ ìˆ˜ì§‘ ì˜ˆì •\n")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = GoogleMapsImageCrawler(headless=False)  # ì²˜ìŒì—ëŠ” headless=Falseë¡œ í™•ì¸ìš©
    
    try:
        total_success = 0
        total_attempted = 0
        
        for i, space in enumerate(spaces_data, 1):
            print(f"\n[{i}/{total_spaces}] ì§„í–‰ ì¤‘...")
            
            success_count = crawler.crawl_place_images(
                place_name=space["name"],
                region=space["region"],
                english_name=space["english_name"],
                max_images=3
            )
            
            total_success += success_count
            total_attempted += 3
            
            # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(2)
            
            # 10ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 10 == 0:
                print(f"\nğŸ“ˆ ì¤‘ê°„ ê²°ê³¼: {total_success}/{total_attempted} ì´ë¯¸ì§€ ìˆ˜ì§‘ ({i}/{total_spaces} ê³µê°„ ì™„ë£Œ)")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    finally:
        crawler.close()
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        success_rate = (total_success / total_attempted * 100) if total_attempted > 0 else 0
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“ˆ ìµœì¢… ì„±ê³µë¥ : {total_success}/{total_attempted} ({success_rate:.1f}%)")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {crawler.download_dir.absolute()}")
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"1. ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ë“¤ì„ í™•ì¸í•˜ì„¸ìš”")
        print(f"2. í’ˆì§ˆì´ ë‚®ì€ ì´ë¯¸ì§€ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”")
        print(f"3. npm run devë¡œ ê°œë°œì„œë²„ë¥¼ ì‹œì‘í•´ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”")

if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = ['selenium', 'requests', 'pandas']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
        for package in missing_packages:
            print(f"   pip install {package}")
        print("\nì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    run_automated_crawling() 